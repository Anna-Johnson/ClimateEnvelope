# load packages
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
require(here)
if (!requireNamespace("rgbif", quietly = TRUE)) install.packages("rgbif")
require(rgbif)
if (!requireNamespace("purrr", quietly = TRUE)) install.packages("purrr")
require(purrr)
if (!requireNamespace("tidyr", quietly = TRUE)) install.packages("tidyr")
require(tidyr)
if (!requireNamespace("sf", quietly = TRUE)) install.packages("sf")
require(sf)
if (!requireNamespace("arcgisbinding", quietly = TRUE)) install.packages("arcgisbinding")
require(arcgisbinding) 

arc.check_product()

here::i_am("Scripting/GBIF_Collector_Refugia.R")

Spp <- read.csv(here("scripting", "SpeciesList_CC_Refugia_Spring2021.csv"))
splist <- Spp$SNAME

# gets the  keys for each species name, based on GBIF
keys <- sapply(splist, function(x) name_backbone(name=x)$speciesKey[1], USE.NAMES=FALSE)

### add some code to put out the list of species not found in GBIF
a1 <- which(sapply(keys,is.null))
missingGBIFsp <- splist[a1]
missingGBIFsp
rm(a1)
cat(length(missingGBIFsp),"of",length(splist), "species were not found in GBIF", "\n", "They are:", paste(missingGBIFsp, collapse=", "))

# gets rid of any null values generated by name_backbone in the case of unmatchable species names
keys1 <- keys[-(which(sapply(keys,is.null),arr.ind=TRUE))] #note: seems to break if there is only one item in the list... use for multiple species!
keys <- keys1
#searches for occurrences
dat <- occ_search(
  taxonKey=keys, 
  limit=10000, # modify if needed, fewer will make testing go faster
  return='data', 
  hasCoordinate=TRUE,
  geometry='POLYGON ((-81.3907145109999 43.022890238,-73.9087973769999 42.922956207,-74.7167285109999 37.083580272, -81.8560689919999 37.390003945, -81.3907145109999 43.022890238))', # simplified boundary of Pennsylvania.
  year='1970,2021'
)

dat <-  dat[dat!="no data found, try a different search"] # deletes the items from the list where no occurences were found. doesn't work for one species
datdf <- lapply(dat, function(x) x[[3]])# selects the $data from each list element

#make the columns all match across the dataframes

#choose which columns you actually care about and will be keeping (these are ragged tables otherwise)
fields <- c('species','scientificName','datasetKey','recordedBy','key','decimalLatitude','decimalLongitude','country','basisOfRecord','coordinateUncertaintyInMeters','coordinateAccuracy','year','month','day')

#not every data frame has every column--this loop adds in the "fields" columns for the dfs that are missing those columns
for (i in 1:length(datdf))    {
  if ((identical(colnames(datdf[[i]]),fields)) == FALSE) {
    nms   = fields
    df =   datdf[[i]]
    aux = colnames(df)
    aux1 = row.names(df)
    
    Missing = setdiff(nms, colnames(df))  
    
    ind = seq(1,length(Missing)) #creating indices 1-5 for loop
    for (j in ind)  {    #loop to add columns with zeros
      df = cbind(df,c(NA))
    }
    colnames(df) = c(aux,Missing)   #updates columns names
    
    df = df[,order(colnames(df))]  #put columns into order
    datdf[[i]] = df              #updates object from list
  } 
}

datdff <- lapply(datdf, '[', fields) #subset out just the focal columns

datdff_df <- dplyr::bind_rows(datdff, .id = "column_label") #turns it into one big dataframe


write.csv(datdff_df, file=paste("gbif", format(Sys.time(), "%Y%m%d"), "backup.csv"), sep="")
##datdf <- read.csv("gbif 20191015 backup.csv", stringsAsFactors=FALSE) # to reload a saved search


gbifdata <- datdff_df # just changing the name so it backs up

gbifdata$DataSource <- "GBIF"

names(gbifdata)[names(gbifdata)=='species'] <- 'SNAME'
names(gbifdata)[names(gbifdata)=='key'] <- 'DataID'
names(gbifdata)[names(gbifdata)=='decimalLongitude'] <- 'Longitude'
names(gbifdata)[names(gbifdata)=='decimalLatitude'] <- 'Latitude'
names(gbifdata)[names(gbifdata)=='year'] <- 'LastObs'
names(gbifdata)[names(gbifdata)=='basisOfRecord'] <- 'Notes'

# pull out records with the least uncertainty
gbifdata <- gbifdata[which(gbifdata$coordinateUncertaintyInMeters<=200|is.na(gbifdata$coordinateUncertaintyInMeters)),]

#subset to the needed columns
gbifdata <- gbifdata[c("SNAME","DataID","DataSource","Notes","LastObs","Longitude","Latitude", "coordinateUncertaintyInMeters")]

#remove rows w/ missing data in coordinates
gbifdata <- gbifdata %>% drop_na(Longitude, Latitude)

# Set cut-off date
cutoffyear <- 1980
gbifdata$usedata <- ifelse(gbifdata$LastObs>=cutoffyear, "y", "n")

# create a spatial layer
proj <- CRS("+proj=lcc +lon_0=-95 +lat_1=49 +lat_2=77 +lat_0=0") #projection of AdaptWest asc layers

gbif_sf <- st_as_sf(gbifdata, coords=c("Longitude","Latitude"), crs="+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
gbif_sf <- st_transform(gbif_sf, crs=proj) # reproject to match the Adaptwest layers

fgdb_path <- file.path("C:/Users/AJohnson/Documents/ArcGIS/Projects/ClimateRefugia/Species"
, "gbif_data.gdb") #create and write to a new file geodatabase

#this creates a points output
arc.write(path=fgdb_path, data=gbif_sf, overwrite=TRUE, shape_info=list(type='Point')) # write a feature class into the geodatabase

#this creates a polygon output
gbif_100mbuffer_sf <- st_buffer(gbif_sf, dist=100) # buffer by 100m
arc.write(path=fgdb_path, data=gbif_100mbuffer_sf, overwrite=TRUE) # write a feature class into the geodatabase
